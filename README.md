# Fooling LIME and SHAP

Post-hoc explanation techniques that rely on input pertubations, such as LIME and SHAP, are not reliable towards systematic errors and underlying biases.
In this project, the scaffolding technique from Slack et al. should be re-implemented, which effectively should hide the biases of any given classifier.
- Paper Reference: https://arxiv.org/abs/1911.02508
- Code Reference: https://github.com/dylan-slack/Fooling-LIME-SHAP


## Installation

```bash
conda env create -f "environment.yml"
conda activate iML-project
```

## Quick Start


## Requirements for Project Subissions
* clean code
* well documented
* unit tested
* all requirements well documented (use requirements.txt)
* Installation instructions (in this README.md)
* If feasible, run your experiments with several random seeds. Try to create reproducabile results.

## Submission Deadline: Feb. 15th 2022, 00:00


